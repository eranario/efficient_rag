{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from transformers import Pipeline\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"naver/splade-v3\"\n",
    "RERANKER_MODEL_NAME = \"colbert-ir/colbertv2.0\"\n",
    "READER_MODEL_NAME = \"naver/cocom-v1-128-mistral-7b\" # max context length: 8192\n",
    "READER_TOKENIZER = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = FAISS.load_local(\"/group/jmearlesgrp/data/KILT/faiss_dbs/faiss_index_large\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = RAGPretrainedModel.from_pretrained(RERANKER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answers = llm.generate_from_text(contexts=[relevant_docs], questions=[question], max_new_tokens=128)\n",
    "\n",
    "    return answers[0], relevant_docs\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_ground_truth(example):\n",
    "    \"\"\"\n",
    "    Extract the ground truth answers from the example.\n",
    "    If the 'answer' field is a dict with an 'aliases' key,\n",
    "    return the full list of aliases.\n",
    "    Otherwise, return the answer as a list.\n",
    "    \"\"\"\n",
    "    gt = example['output'][0].get(\"answer\", \"\")\n",
    "    if isinstance(gt, dict) and \"aliases\" in gt:\n",
    "        return gt[\"aliases\"]\n",
    "    elif isinstance(gt, list):\n",
    "        return gt\n",
    "    else:\n",
    "        return [gt]\n",
    "\n",
    "def exact_match_score(prediction, ground_truth_aliases):\n",
    "    \"\"\"\n",
    "    Returns 1 if the normalized prediction exactly matches any alias in\n",
    "    ground_truth_aliases, otherwise returns 0.\n",
    "    \"\"\"\n",
    "    norm_pred = normalize_answer(prediction)\n",
    "    for alias in ground_truth_aliases:\n",
    "        if norm_pred == normalize_answer(alias):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset to evaluate\n",
    "dataset = datasets.load_dataset(\"facebook/kilt_tasks\", \"hotpotqa\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_em = 0\n",
    "total = 0\n",
    "\n",
    "for example in tqdm(dataset, desc=\"Evaluating EM\"):\n",
    "    question = example.get(\"input\", \"\")\n",
    "    ground_truth_aliases = get_ground_truth(example)  # full list of aliases\n",
    "\n",
    "    prediction, used_docs = answer_with_rag(\n",
    "        question,\n",
    "        reader_model,\n",
    "        vdb,\n",
    "        reranker=reranker,\n",
    "        num_retrieved_docs=30,\n",
    "        num_docs_final=5,\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Ground Truth Aliases: {ground_truth_aliases}\")\n",
    "    print(f\"Used Documents: {used_docs}\")\n",
    "    total_em += exact_match_score(prediction, ground_truth_aliases)\n",
    "    total += 1\n",
    "    \n",
    "    break\n",
    "\n",
    "em_score = total_em / total if total > 0 else 0\n",
    "print(f\"Exact Match (EM) score: {em_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
